{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion: The Bridge Between Text and Image Modalities\n",
    "## Author: [Your Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In the rapidly evolving landscape of artificial intelligence, few developments have captured the public imagination quite like text-to-image generation models. Among these, Stable Diffusion stands out as a pivotal breakthrough that has democratized access to high-quality image generation.\n",
    "\n",
    "My fascination with Stable Diffusion stems from its elegant solution to a complex problem: how can we translate the rich semantic meaning of language into coherent visual representations? This question lies at the heart of multimodal learning—the frontier where different forms of data interpretation converge.\n",
    "\n",
    "When Stability AI released Stable Diffusion as an open-source model in August 2022, it marked a significant departure from the closed ecosystems that had dominated AI image generation. Suddenly, anyone with modest computing resources could generate sophisticated imagery from text descriptions. This democratization sparked an explosion of creativity and innovation that continues to reverberate through art, design, and technology communities.\n",
    "\n",
    "What drew me to explore this technology isn't just its impressive output quality, but the fascinating technical approach that makes it possible. Stable Diffusion's use of latent diffusion—operating in a compressed representation space rather than directly on pixels—represents an elegant solution to the computational challenges of image generation. The model's ability to bridge textual concepts and visual elements embodies the promise of multimodal AI: systems that can fluidly translate between different forms of information much as humans do.\n",
    "\n",
    "In this exploration, I'll dive into how Stable Diffusion works, examine its capabilities and limitations, and reflect on its significance in the broader evolution of multimodal learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Context: The Evolution of Text-to-Image Generation\n",
    "\n",
    "To appreciate Stable Diffusion's significance, we need to understand its place in the evolution of text-to-image systems. This history reveals a fascinating progression of approaches to the multimodal challenge of translating words into visuals:\n",
    "\n",
    "### Early GAN-Based Approaches (2016-2020)\n",
    "\n",
    "The modern era of text-to-image synthesis began with Generative Adversarial Networks (GANs). Models like Reed et al.'s GAN-CLS (2016) and Zhang et al.'s StackGAN (2017) pioneered conditioning image generation on text descriptions. However, these early systems struggled with coherence and detail, often producing blurry or distorted results that only vaguely matched the input text.\n",
    "\n",
    "These early models highlighted a fundamental challenge in multimodal learning: creating a shared semantic space that could meaningfully connect the statistical patterns of language with the visual features of images. The results were promising but limited—like an artist who could grasp the basic subject of a description but lacked the skill to render it convincingly.\n",
    "\n",
    "### The DALL-E Era (2021)\n",
    "\n",
    "OpenAI's introduction of DALL-E in January 2021 represented a quantum leap forward. By leveraging a transformer-based architecture (similar to GPT-3) and training on 250 million text-image pairs, DALL-E demonstrated a remarkable ability to generate coherent images from complex prompts. It could handle abstract concepts, spatial relationships, and even some aspects of visual reasoning.\n",
    "\n",
    "DALL-E's capabilities highlighted the power of large-scale training and transformer architectures for bridging modalities. However, its closed nature and massive computational requirements limited broader experimentation and application.\n",
    "\n",
    "### The Diffusion Revolution (2021-2022)\n",
    "\n",
    "While GANs dominated early text-to-image work, researchers were exploring an alternative approach: diffusion models. These models work by gradually removing noise from a random initial state, guided by the input text. OpenAI's GLIDE (2021) demonstrated that diffusion models could match or exceed GAN performance for text-to-image generation.\n",
    "\n",
    "The diffusion approach offered several advantages: more stable training, better diversity, and improved handling of complex compositions. However, the computational cost remained prohibitive—until Stable Diffusion changed the game.\n",
    "\n",
    "### Stable Diffusion: The Open Revolution (2022)\n",
    "\n",
    "In August 2022, Stability AI, in collaboration with researchers from LMU Munich and Runway, released Stable Diffusion. Building on the latent diffusion approach introduced by Rombach et al., Stable Diffusion operated in a compressed latent space rather than pixel space, dramatically reducing computational requirements while maintaining quality.\n",
    "\n",
    "Crucially, Stable Diffusion was released as an open-source model with a permissive license, allowing researchers, developers, and artists to run, modify, and build upon it. This openness catalyzed an unprecedented wave of innovation: within months, the community had developed techniques for image editing, personalization, animation, and much more.\n",
    "\n",
    "### Recent Developments (2023-2025)\n",
    "\n",
    "The pace of advancement has only accelerated since Stable Diffusion's release. Improved versions like SDXL have pushed quality boundaries, while specialized extensions like ControlNet have enabled more precise control over generation. The integration of diffusion models with other AI systems has enabled new multimodal applications from video generation to 3D synthesis.\n",
    "\n",
    "Throughout this evolution, we've seen consistent progress toward more seamless translation between language and vision—the essence of multimodal learning. Stable Diffusion represents a pivotal moment in this journey, where powerful multimodal capabilities became accessible to a global community of creators and developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Stable Diffusion's Architecture\n",
    "\n",
    "At its core, Stable Diffusion represents a fascinating approach to bridging text and image modalities. Let's explore the key components that make this possible:\n",
    "\n",
    "### The Multimodal Foundation\n",
    "\n",
    "Stable Diffusion integrates three primary components that work together to translate text into images:\n",
    "\n",
    "1. **Text Encoder (CLIP)**: Developed by OpenAI, the Contrastive Language-Image Pre-training (CLIP) model serves as the bridge between text and visual concepts. When you enter a prompt, CLIP encodes it into a high-dimensional embedding that captures the semantic meaning of your description.\n",
    "\n",
    "2. **U-Net Denoiser with Cross-Attention**: The heart of the diffusion process, this component gradually transforms random noise into a coherent image. Critically, it uses cross-attention mechanisms to incorporate the text embeddings from CLIP, ensuring the emerging image aligns with your prompt.\n",
    "\n",
    "3. **Variational Autoencoder (VAE)**: This component compresses images into a lower-dimensional latent space and reconstructs them back into pixel space. By operating in this compressed latent space, Stable Diffusion achieves significant efficiency gains compared to earlier diffusion models.\n",
    "\n",
    "### The Latent Diffusion Process\n",
    "\n",
    "Stable Diffusion's name comes from its use of diffusion models—a class of generative models that work by reversing a gradual noising process. Here's how the generation process unfolds:\n",
    "\n",
    "1. Start with pure noise in the latent space\n",
    "2. Gradually denoise this representation over multiple steps\n",
    "3. Guide this denoising process using the text embeddings from CLIP\n",
    "4. Once denoising is complete, decode the latent representation to a full image\n",
    "\n",
    "What makes this approach particularly elegant is that it operates in the compressed latent space rather than pixel space, reducing the computational burden by orders of magnitude while preserving the model's ability to generate detailed images.\n",
    "\n",
    "### The Multimodal Magic: Cross-Attention\n",
    "\n",
    "The true multimodal innovation in Stable Diffusion lies in its cross-attention mechanism. This allows the model to create connections between specific words in your prompt and visual elements in the generated image. For example, when you write \"a red car on a beach at sunset,\" cross-attention helps the model associate \"red\" with the color of the car, \"beach\" with sandy textures, and \"sunset\" with warm lighting conditions.\n",
    "\n",
    "This mapping between language and visual concepts is learned during training on millions of image-text pairs, creating a rich semantic space where textual and visual information can interact. The result is a system that doesn't just generate images that vaguely match a description, but one that can understand and implement specific visual attributes, compositions, and styles mentioned in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "Before we dive into experimenting with Stable Diffusion, let's set up our environment with the necessary libraries. This code installs the required packages and imports the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q diffusers==0.24.0 transformers==4.35.2 torch accelerate scipy ftfy safetensors\n",
    "!pip install -q matplotlib numpy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from diffusers import EulerDiscreteScheduler, DDIMScheduler, DPMSolverMultistepScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Helper function to create output directories\n",
    "def create_output_dir(dir_name=\"outputs\"):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "output_dir = create_output_dir()\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the Stable Diffusion model. We'll use version 1.5, which balances quality and efficiency well. This step might take a few minutes as it downloads the model weights (about 4GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"  # We're using v1.5 for this exploration\n",
    "\n",
    "# Load the pipeline with a scheduler optimized for quality\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable memory efficient attention if on CUDA (for better performance)\n",
    "if device == \"cuda\":\n",
    "    pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diffusion Journey: From Noise to Meaning\n",
    "\n",
    "One of the most fascinating aspects of diffusion models is how they gradually transform pure noise into coherent visual content. Unlike GANs, which generate images in a single step, diffusion models work iteratively—starting with randomness and progressively removing noise while injecting structure guided by the text prompt.\n",
    "\n",
    "Let's visualize this process to better understand the journey from chaos to meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_diffusion_process(prompt, num_steps=20):\n",
    "    \"\"\"Visualize the diffusion process from noise to final image\"\"\"\n",
    "    # Store the original callback\n",
    "    orig_callback = pipe.callback\n",
    "    \n",
    "    # List to store intermediate images\n",
    "    intermediate_images = []\n",
    "    \n",
    "    # Custom callback to capture intermediate results\n",
    "    def callback_fn(i, t, latents):\n",
    "        # Decode the latents to get the intermediate image\n",
    "        with torch.no_grad():\n",
    "            latents_input = 1 / 0.18215 * latents\n",
    "            image = pipe.vae.decode(latents_input).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            intermediate_images.append((i, image))\n",
    "        return True\n",
    "    \n",
    "    # Set our callback\n",
    "    pipe.callback = callback_fn\n",
    "    \n",
    "    # Generate the image\n",
    "    print(f\"Generating image for: '{prompt}'\")\n",
    "    generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "    _ = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    # Restore the original callback\n",
    "    pipe.callback = orig_callback\n",
    "    \n",
    "    # Plot a selection of the intermediate results\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    # Select steps to show (beginning, end, and some middle steps)\n",
    "    indices_to_show = [0, 1, 2, 3, 5, 7, 9, 12, 15, 19] if num_steps >= 20 else list(range(num_steps))\n",
    "    indices_to_show = [idx for idx in indices_to_show if idx < len(intermediate_images)]\n",
    "    \n",
    "    for i, idx in enumerate(indices_to_show):\n",
    "        step, img = intermediate_images[idx]\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Step {step}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"The Diffusion Journey: '{prompt}'\\nFrom Random Noise to Structured Image\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/diffusion_journey.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate and return the final image for display\n",
    "    return pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "\n",
    "# Let's observe the diffusion process for a landscape prompt\n",
    "journey_prompt = \"A serene mountain lake reflecting the stars and aurora borealis\"\n",
    "final_image = visualize_diffusion_process(journey_prompt)\n",
    "\n",
    "# Display the final image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.array(final_image))\n",
    "plt.title(f\"Final Result: '{journey_prompt}'\")\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"{output_dir}/final_image.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Diffusion Process\n",
    "\n",
    "The visualization above reveals the fascinating transformation that occurs during the diffusion process. Several key insights emerge:\n",
    "\n",
    "1. **From Chaos to Order**: In the earliest steps, the image is pure noise—random patterns with no discernible structure. This represents maximum entropy, a state of complete uncertainty.\n",
    "\n",
    "2. **Emergence of Structure**: By the middle steps, broad structures begin to form. The model first establishes the basic composition—separating sky from mountains and water—before adding details.\n",
    "\n",
    "3. **Progressive Refinement**: Details emerge in order of importance. Major elements like the mountains and aurora appear first, followed by finer details like reflections and textures.\n",
    "\n",
    "4. **Coherent Interpretation**: The final image represents the model's coherent interpretation of the prompt, integrating multiple concepts (mountains, lake, stars, aurora) into a unified visual scene.\n",
    "\n",
    "This progressive refinement mirrors how human artists often work—starting with rough compositions and gradually adding detail. The difference is that Stable Diffusion begins with noise rather than a blank canvas, and removes uncertainty rather than adding information.\n",
    "\n",
    "From a multimodal learning perspective, what's remarkable is how the text prompt guides this entire process. The CLIP text embeddings influence each denoising step, ensuring the emerging image aligns with the semantic content of your words. This represents a deep integration of language understanding and visual generation—the essence of multimodal intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Language of Images: Exploring Prompt Engineering\n",
    "\n",
    "One of the most fascinating aspects of text-to-image systems is how the specific language of prompts affects the generated images. This discipline—prompt engineering—has emerged as a crucial skill for effectively communicating with AI systems.\n",
    "\n",
    "Let's explore how different formulations of prompts influence Stable Diffusion's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_prompt_variations(prompts):\n",
    "    \"\"\"Generate images for different prompt variations\"\"\"\n",
    "    images = []\n",
    "    \n",
    "    # Generate images for each prompt\n",
    "    for prompt in prompts:\n",
    "        generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "        print(f\"Generating: '{prompt}'\")\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=30,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "        images.append(image)\n",
    "    \n",
    "    # Create a figure to display the results\n",
    "    rows = len(prompts)\n",
    "    plt.figure(figsize=(12, 5 * rows))\n",
    "    for i, (prompt, image) in enumerate(zip(prompts, images)):\n",
    "        plt.subplot(rows, 1, i + 1)\n",
    "        plt.imshow(np.array(image))\n",
    "        plt.title(f\"Prompt: '{prompt}'\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/prompt_variations.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Let's explore variations on a simple concept: a castle\n",
    "castle_prompts = [\n",
    "    \"castle\",  # Minimal prompt\n",
    "    \"a medieval castle\",  # Adding context\n",
    "    \"a detailed photograph of a medieval castle on a hill\",  # Adding medium and setting\n",
    "    \"a highly detailed medieval castle on a hill, moat, drawbridge, sunset, dramatic lighting, 8k, professional photography\"  # Elaborated prompt with style cues\n",
    "]\n",
    "\n",
    "explore_prompt_variations(castle_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Art of Prompt Engineering\n",
    "\n",
    "The examples above reveal several key principles of effective prompt engineering:\n",
    "\n",
    "1. **Specificity Matters**: Notice how the minimal prompt \"castle\" produces a basic, generic castle, while more specific prompts yield richer, more contextualized results. This demonstrates the model's ability to incorporate additional context into its visual interpretation.\n",
    "\n",
    "2. **Medium and Style Guidance**: When we mention \"photograph\" or \"professional photography,\" the model generates images with photographic qualities—proper lighting, perspective, and detail. This shows how the model has learned associations between textual style descriptions and visual characteristics.\n",
    "\n",
    "3. **Compositional Elements**: Adding elements like \"on a hill,\" \"moat,\" and \"drawbridge\" influences not just what appears in the image but how these elements are arranged. This reveals the model's understanding of spatial relationships described in language.\n",
    "\n",
    "4. **Technical Cues**: Terms like \"8k\" and \"highly detailed\" signal to the model that you want more intricate textures and details, showing how the model has learned to associate technical photography terminology with visual qualities.\n",
    "\n",
    "From a multimodal learning perspective, what's happening here is remarkable: the model is translating not just the content of language (what objects should appear) but also the nuance, style, and technical aspects described in text. This demonstrates a deep integration between linguistic understanding and visual representation.\n",
    "\n",
    "Prompt engineering has emerged as a new form of literacy—a way of communicating with AI systems that requires understanding how they interpret language and translate it to other modalities. It's a perfect example of the human-AI interface that develops around multimodal systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power of Negative Space: Understanding Negative Prompts\n",
    "\n",
    "One of the most powerful techniques in Stable Diffusion is the use of negative prompts—text descriptions of what you don't want to appear in the generated image. This feature demonstrates the model's ability to understand not just what to include, but what to avoid.\n",
    "\n",
    "Let's explore how negative prompts influence the generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_negative_prompts(prompt, negative_prompts):\n",
    "    \"\"\"Explore how negative prompts affect image generation\"\"\"\n",
    "    images = []\n",
    "    titles = []\n",
    "    \n",
    "    # First, generate an image without a negative prompt\n",
    "    generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "    print(f\"Generating base image for: '{prompt}'\")\n",
    "    base_image = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=30,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    images.append(base_image)\n",
    "    titles.append(f\"Base: '{prompt}'\\nNegative: None\")\n",
    "    \n",
    "    # Generate images with each negative prompt\n",
    "    for neg_prompt in negative_prompts:\n",
    "        generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "        print(f\"Generating with negative: '{neg_prompt}'\")\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=neg_prompt,\n",
    "            num_inference_steps=30,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "        images.append(image)\n",
    "        titles.append(f\"Base: '{prompt}'\\nNegative: '{neg_prompt}'\")\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(15, 5 * ((len(images) + 1) // 2)))\n",
    "    for i, (image, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot((len(images) + 1) // 2, 2, i + 1)\n",
    "        plt.imshow(np.array(image))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/negative_prompts.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Let's explore negative prompts with a portrait example\n",
    "portrait_prompt = \"A portrait of a person in an ornate costume, detailed face, studio lighting\"\n",
    "negative_prompts = [\n",
    "    \"blurry, low quality, distorted, deformed\",  # Quality negatives\n",
    "    \"dark, moody, shadows, low-key lighting\",  # Lighting negatives\n",
    "    \"multiple people, crowd, busy background\",  # Composition negatives\n",
    "    \"modern clothing, casual, contemporary\"  # Style negatives\n",
    "]\n",
    "\n",
    "explore_negative_prompts(portrait_prompt, negative_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Significance of Negative Prompts\n",
    "\n",
    "The examples above reveal several fascinating insights about negative prompts and their role in multimodal systems:\n",
    "\n",
    "1. **Selective Constraint**: Negative prompts don't just remove specific objects; they guide the overall aesthetic direction. For instance, negating \"dark, moody\" pushes the model toward brighter, more evenly lit compositions.\n",
    "\n",
    "2. **Clarifying Ambiguity**: Natural language is inherently ambiguous. When we say \"portrait,\" that could include multiple interpretations. Negative prompts help narrow the possibility space by explicitly excluding certain interpretations.\n",
    "\n",
    "3. **Style Direction**: By negating certain styles (like \"modern clothing\"), we can push the model toward others (like historical or fantasy costumes) without having to explicitly describe every aspect of the desired style.\n",
    "\n",
    "4. **Quality Control**: Technical negatives like \"blurry, low quality\" help push the model toward higher-quality renderings, showing that the model has internalized concepts of image quality.\n",
    "\n",
    "From a multimodal learning perspective, negative prompts demonstrate something profound about how these models work. They don't just learn positive associations between text and images; they also learn what visual features correspond to certain negative descriptions. This dual capability—understanding both what something is and what it isn't—mirrors human conceptual thinking.\n",
    "\n",
    "The concept of negative prompts has no direct analogue in traditional art or design tools. It represents a new paradigm in human-AI interaction, where we guide systems not just by telling them what to do, but also what to avoid—shaping the generation process through both positive and negative constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossing Modalities: Image-to-Image Translation\n",
    "\n",
    "So far, we've explored how Stable Diffusion translates text into images. But one of the most powerful aspects of multimodal systems is their ability to translate between multiple forms of data. Let's explore how Stable Diffusion can transform existing images based on text prompts—a capability called image-to-image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the image-to-image pipeline\n",
    "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "img2img_pipe = img2img_pipe.to(device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    img2img_pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "def demonstrate_img2img_transformation(base_prompt, transformation_prompts, strength=0.75):\n",
    "    \"\"\"Demonstrate image-to-image transformation using text prompts\"\"\"\n",
    "    # First, generate a base image\n",
    "    generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "    print(f\"Generating base image: '{base_prompt}'\")\n",
    "    init_image = pipe(\n",
    "        prompt=base_prompt,\n",
    "        num_inference_steps=30,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    # Store all images and their descriptions\n",
    "    all_images = [init_image]\n",
    "    all_titles = [f\"Original: '{base_prompt}'\"]\n",
    "    \n",
    "    # Apply each transformation prompt\n",
    "    for transform_prompt in transformation_prompts:\n",
    "        generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "        print(f\"Transforming to: '{transform_prompt}'\")\n",
    "        transformed_image = img2img_pipe(\n",
    "            prompt=transform_prompt,\n",
    "            image=init_image,\n",
    "            strength=strength,  # How much to transform (0-1)\n",
    "            guidance_scale=7.5,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "        \n",
    "        all_images.append(transformed_image)\n",
    "        all_titles.append(f\"Transformed: '{transform_prompt}'\")\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(15, 5 * ((len(all_images) + 1) // 2)))\n",
    "    for i, (image, title) in enumerate(zip(all_images, all_titles)):\n",
    "        plt.subplot((len(all_images) + 1) // 2, 2, i + 1)\n",
    "        plt.imshow(np.array(image))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/img2img_transformation.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Demonstrate transformations from a landscape to different seasons/styles\n",
    "base_landscape = \"A peaceful valley with mountains in the background and a river\"\n",
    "transformations = [\n",
    "    \"A snowy winter valley with frozen river and mountains in the background\",\n",
    "    \"An autumn valley with colorful foliage, mountains in the background and a river with fallen leaves\",\n",
    "    \"A mystical fantasy valley with glowing elements, ethereal lighting, magical atmosphere\"\n",
    "]\n",
    "\n",
    "demonstrate_img2img_transformation(base_landscape, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Significance of Image-to-Image Translation\n",
    "\n",
    "The image-to-image transformation capability reveals several important aspects of multimodal systems:\n",
    "\n",
    "1. **Content vs. Style Separation**: Notice how the basic composition (mountains, valley, river) persists across transformations, while style elements (season, lighting, atmosphere) change. This demonstrates the model's ability to distinguish between content and stylistic elements.\n",
    "\n",
    "2. **Semantic Understanding**: When we transform to \"winter,\" the model doesn't just make everything white—it understands the concept of winter includes frozen water, snow on mountains, and certain lighting conditions. This shows the model has learned semantic associations between concepts and their visual manifestations.\n",
    "\n",
    "3. **Controlled Transformation**: The \"strength\" parameter allows us to control how much of the original image persists versus how much is guided by the new prompt. This represents a blending of information from the image modality and the text modality.\n",
    "\n",
    "4. **Creative Recombination**: The fantasy transformation shows how the model can take an existing scene and reimagine it in a completely different aesthetic context while maintaining the core structural elements.\n",
    "\n",
    "From a multimodal learning perspective, image-to-image transformation represents a sophisticated form of cross-modal translation. The system must:\n",
    "\n",
    "1. Understand the content and structure of the input image\n",
    "2. Interpret the semantic meaning of the transformation prompt\n",
    "3. Selectively modify aspects of the image to align with the prompt\n",
    "4. Preserve appropriate elements from the original image\n",
    "\n",
    "This capability has profound implications for creative workflows, allowing artists and designers to rapidly explore variations and alternatives based on natural language guidance. It also demonstrates how multimodal systems can serve as bridges between different forms of media, translating intent expressed in one modality (text) to modifications in another (images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peering Into the Model's Mind: Visualizing Text Embeddings\n",
    "\n",
    "To truly understand how Stable Diffusion bridges text and image modalities, it's illuminating to explore how the model represents text internally. The CLIP text encoder transforms textual prompts into high-dimensional embeddings that guide the diffusion process. Let's visualize these embeddings to gain insight into the model's semantic space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_text_embeddings(prompts):\n",
    "    \"\"\"Visualize text embeddings using t-SNE dimensional reduction\"\"\"\n",
    "    # Access the text encoder and tokenizer from the pipeline\n",
    "    text_encoder = pipe.text_encoder\n",
    "    tokenizer = pipe.tokenizer\n",
    "    \n",
    "    # Compute embeddings for each prompt\n",
    "    embeddings = []\n",
    "    \n",
    "    print(\"Computing text embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            # Tokenize the text\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # Move inputs to the same device as the model\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Get embeddings from the text encoder\n",
    "            outputs = text_encoder(inputs.input_ids)\n",
    "            # Get the pooled output as the embedding vector\n",
    "            embedding = outputs[0].mean(dim=1).cpu().numpy()[0]\n",
    "            embeddings.append(embedding)\n",
    "    \n",
    "    # Use t-SNE to reduce dimensions for visualization\n",
    "    print(\"Reducing dimensions with t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=SEED, perplexity=min(len(prompts)-1, 5))\n",
    "    embeddings_2d = tsne.fit_transform(np.array(embeddings))\n",
    "    \n",
    "    # Create categories for prompts\n",
    "    categories = [prompt.split()[0] for prompt in prompts]  # Use first word as category\n",
    "    unique_categories = list(set(categories))\n",
    "    category_to_color = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "    colors = [category_to_color[cat] for cat in categories]\n",
    "    \n",
    "    # Plot the embeddings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, cmap='viridis', alpha=0.8, s=100)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, (prompt, coord) in enumerate(zip(prompts, embeddings_2d)):\n",
    "        # Shorten very long prompts for readability\n",
    "        display_prompt = prompt if len(prompt) < 40 else prompt[:37] + \"...\"\n",
    "        plt.annotate(\n",
    "            display_prompt,\n",
    "            xy=(coord[0], coord[1]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    # Add a legend for categories\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(category_to_color[cat]/len(unique_categories)), \n",
    "                          markersize=10, label=cat) for cat in unique_categories]\n",
    "    plt.legend(handles=handles, title=\"Categories\")\n",
    "    \n",
    "    plt.title(\"Visualizing Text Embeddings in CLIP's Semantic Space\", fontsize=16)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f\"{output_dir}/text_embeddings.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Prepare a diverse set of prompts to explore the semantic space\n",
    "diverse_prompts = [\n",
    "    # Nature scenes\n",
    "    \"A mountain landscape with snow peaks\",\n",
    "    \"A lush green forest with sunlight filtering through trees\",\n",
    "    \"A tranquil beach at sunset with waves\",\n",
    "    \"A desert landscape with sand dunes\",\n",
    "    # Urban environments\n",
    "    \"A bustling cityscape at night with skyscrapers\",\n",
    "    \"A cyberpunk city with neon lights and rain\",\n",
    "    \"A medieval town with stone buildings and narrow streets\",\n",
    "    \"A futuristic metropolis with flying vehicles\",\n",
    "    # Fantasy concepts\n",
    "    \"A magical floating island with waterfalls\",\n",
    "    \"A dragon perched on a castle tower\",\n",
    "    \"A wizard casting a spell with glowing energy\",\n",
    "    \"A fairy forest with glowing mushrooms and ethereal light\"\n",
    "]\n",
    "\n",
    "visualize_text_embeddings(diverse_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Text Embedding Visualization\n",
    "\n",
    "The visualization of text embeddings reveals fascinating insights into how Stable Diffusion understands language and connects it to visual concepts:\n",
    "\n",
    "1. **Semantic Clustering**: Notice how similar concepts cluster together in the embedding space. Nature scenes tend to group near each other, as do urban environments and fantasy concepts. This demonstrates that CLIP has learned meaningful semantic relationships between different textual descriptions.\n",
    "\n",
    "2. **Gradient of Similarity**: The distance between points represents semantic similarity. Concepts that share elements (like \"mountain landscape\" and \"forest\") are closer together than wholly different concepts (like \"cityscape\" and \"dragon\").\n",
    "\n",
    "3. **Conceptual Bridges**: Some prompts act as bridges between conceptual clusters. For instance, \"medieval town\" might sit between urban environments and fantasy concepts, sharing elements of both.\n",
    "\n",
    "4. **Multi-Dimensional Understanding**: While we've reduced the embeddings to 2D for visualization, the actual embedding space is 768-dimensional, allowing for rich, nuanced representations of semantic relationships.\n",
    "\n",
    "From a multimodal perspective, these embeddings represent the critical bridge between language and vision. When the diffusion model uses these embeddings to guide image generation, it's essentially navigating through this semantic space—moving toward regions that align with the desired concepts and away from unrelated areas.\n",
    "\n",
    "This visualization helps us understand why certain prompts produce similar images, why small changes in wording can significantly alter results, and how the model connects abstract textual concepts to visual elements. The semantic space shaped by CLIP's training on millions of text-image pairs serves as the foundation for Stable Diffusion's remarkable ability to translate between modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections: Surprises and Insights\n",
    "\n",
    "After exploring Stable Diffusion through the lens of multimodal learning, several aspects stand out as particularly surprising and insightful:\n",
    "\n",
    "### What Surprised Me\n",
    "\n",
    "1. **The Power of Implicit Knowledge**: Throughout this exploration, I've been struck by how much implicit knowledge Stable Diffusion has absorbed about the world. It understands that forests have trees, that cyberpunk involves neon lights, that sunset creates warm lighting—none of which is explicitly stated in many prompts. This emergent knowledge suggests that the model has learned deep connections between concepts across modalities.\n",
    "\n",
    "2. **The Importance of Prompting Language**: The dramatic difference between minimal prompts and detailed ones revealed that the model's output quality isn't just a function of its technical capabilities, but also of how effectively we communicate with it. This human-AI interaction layer represents a new form of literacy that bridges human language and machine understanding.\n",
    "\n",
    "3. **Compositional Understanding**: Stable Diffusion doesn't just understand individual concepts but their relationships. It can place a castle on a hill, with a moat around it, and mountains in the background—showing a sophisticated grasp of spatial relationships and composition that emerges from its multimodal training.\n",
    "\n",
    "4. **Style Transfer Capabilities**: The model's ability to apply consistent stylistic elements across different subjects demonstrates that it has learned abstract visual concepts like \"ethereal,\" \"cinematic,\" or \"photorealistic\"—showing how it bridges semantic concepts and visual aesthetics.\n",
    "\n",
    "5. **The Impact of Open Access**: Perhaps most surprising is how the open-source nature of Stable Diffusion accelerated innovation. Within months of its release, the community had developed extensions, optimizations, and applications that might have taken years in a closed ecosystem. This democratized approach to AI has profound implications for the future of technology development.\n",
    "\n",
    "### Scope for Improvement\n",
    "\n",
    "Despite its impressive capabilities, Stable Diffusion still has significant limitations and areas for growth:\n",
    "\n",
    "1. **Understanding Complex Instructions**: The model sometimes struggles with prompts that involve multiple subjects with specific relationships or complicated actions. Improving the understanding of structured relationships in text would enhance its multimodal capabilities.\n",
    "\n",
    "2. **Consistency Across Elements**: Maintaining consistency in lighting, perspective, and style across all elements of an image remains challenging, particularly in complex scenes. Enhanced cross-attention mechanisms could improve this aspect.\n",
    "\n",
    "3. **Ethical and Social Implications**: The current implementation raises concerns about bias, misrepresentation, and the potential for misuse. Future developments should incorporate stronger ethical guardrails while preserving creative flexibility.\n",
    "\n",
    "4. **Integration with Other Modalities**: While Stable Diffusion excels at text-to-image and image-to-image translation, integrating with other modalities like audio, video, and 3D would create even more powerful multimodal systems.\n",
    "\n",
    "5. **Computational Efficiency**: Though more efficient than many predecessors, Stable Diffusion still requires significant computational resources. Further optimization could make these capabilities accessible on more devices and to more users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: The Significance of Stable Diffusion in Multimodal Learning\n",
    "\n",
    "Stable Diffusion represents a landmark achievement in multimodal learning—a system that bridges the gap between textual understanding and visual creation in a way that feels almost magical. Through our exploration, we've seen how it transforms abstract language into concrete imagery, how it interprets stylistic nuances, and how it navigates a rich semantic space connecting words and visual concepts.\n",
    "\n",
    "The significance of Stable Diffusion extends beyond its impressive outputs. It demonstrates several key principles that will likely shape the future of multimodal AI:\n",
    "\n",
    "1. **The Power of Shared Semantic Spaces**: By learning shared representations across modalities, systems can translate between them in rich, nuanced ways. The CLIP embedding space serves as a kind of universal translator between language and vision.\n",
    "\n",
    "2. **The Importance of Architectural Innovation**: The latent diffusion approach—working in a compressed space rather than directly with pixels—showcases how architectural innovations can dramatically improve efficiency while maintaining quality.\n",
    "\n",
    "3. **The Value of Open Systems**: Stable Diffusion's open-source nature catalyzed an explosion of innovation, extensions, and applications. This demonstrates how open approaches can accelerate progress in complex domains.\n",
    "\n",
    "4. **The Evolution of Human-AI Interaction**: Prompt engineering has emerged as a crucial skill—a new form of literacy that bridges human intention and AI understanding. This highlights how multimodal systems create new interfaces between humans and technology.\n",
    "\n",
    "5. **The Blurring of Creation and Curation**: With systems like Stable Diffusion, the line between creating content and curating it begins to blur. The creative process becomes more conversational—a dialogue between human intention and machine interpretation.\n",
    "\n",
    "As we look to the future, Stable Diffusion provides a glimpse of where multimodal AI is heading: toward systems that can fluidly translate between diverse forms of information, understand implicit knowledge across domains, and collaborate with humans in increasingly intuitive ways. The bridge between language and vision built by Stable Diffusion is just the beginning of what promises to be a transformative journey in human-AI collaboration and creative expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 10684-10695).\n",
    "\n",
    "2. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. *Advances in Neural Information Processing Systems, 33*, 6840-6851.\n",
    "\n",
    "3. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. *International Conference on Machine Learning* (pp. 8748-8763).\n",
    "\n",
    "4. Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., ... & Chen, M. (2021). GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741*.\n",
    "\n",
    "5. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with CLIP latents. *arXiv preprint arXiv:2204.06125*.\n",
    "\n",
    "6. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. N. (2017). StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. *Proceedings of the IEEE International Conference on Computer Vision* (pp. 5907-5915).\n",
    "\n",
    "7. Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *arXiv preprint*.\n",
    "\n",
    "8. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., & Lee, H. (2016). Generative adversarial text to image synthesis. *International Conference on Machine Learning* (pp. 1060-1069).\n",
    "\n",
    "9. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. *arXiv preprint arXiv:2205.11487*.\n",
    "\n",
    "10. Hugging Face Diffusers Library Documentation: https://huggingface.co/docs/diffusers/\n",
    "\n",
    "11. Stability AI GitHub Repository: https://github.com/Stability-AI/stablediffusion\n",
    "\n",
    "12. CompVis GitHub Repository: https://github.com/CompVis/stable-diffusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}